--- Sequence to Sequence Models ---
    .eg: machine translation, 

    -- Basic Models --
        
        - Encoder-Decoder Model -
            . use an RNN to create a "encoded" vector from an input, and use a RNN to decode it 
                -> encode setence in french and decode to english
                -> image captioning, encode image (using a CNN, like AlexNet to encode the image in a feature vector) and decode to the caption

            . decode takes as input initially the encoded vector (for a0) and then the previous y^
        
     -- Picking the most Likely Sentence --
        . instead if picking a random output like done previously for text generation

            -> Language model: modeling the probabilities of the every next possible word (depending on the previous ones), and sample at random from the distribution

            -> Machine Translation: estimate the probability of the output sentence conditioned by an input sentence, and sample the most probable (the best one, the max, deterministic behaviour)

                -> To find the max, dont use Greedy Search
                    . will be stuck in local minima, because it will the best first word and so one....dont explore all the space

                ->> Use BEAM Search

        
        -- Beam Search --
            . broad algorithm to find the output that maximizes the probability
            . approximate search algorithm (search space is too big for a brute force search)

            i. Try to pick the first element
                . P(y<1>|x), for the input sequence X, get the probability of the fisrt element y^ (from the softmax output of all the options)
                    . greedy search would only pick the highest prob
                    . beam search: will select multiple best contenders (number defined by B -> beam width)
                        -> B = 1, is equal to greedy search

            ii. for each previous word from the prev beam search step, select the next most probable 'B' words
                P(y<1>,y<2>|x) = p(y<1>|x) * p(y<2>|x,y<1>) -> the probability of pair of the first and second word (which is equal to the prod of the two seperate conditional probabilities)

            iii. repeat ii. for the next words, going in a breath first manner

            iv. pick the 3 more probable sub-sequences out of these previous evaluations and repeat (only with 3 options)

            -> Number of combinations evaluated = vocab_size * beam_width

            v. when you reach a sequence end condition (<EOS>), store this sequence
            
            vi. when all the final sequences have been evaluated and stored, choose the one with the highest probability!!

            - Tricks and Tips -
                -> Length Normalization:
                    . the probabilities for the subsequences as they are calculated as products of conditional probabilities (small fraction number) can result in a very very small number
                        -> Numerical Underflow: too small for the float representation to represent with good accuracy

                        solution: instead of using a product (PI), use a sum of the logs of the conditional probabilities!
                            -> results in  a more numerical stable algorithm, with less rounding errors

                    -> Length problem
                    . Because the result of a probability of a sequence made by multiple conditional probs (which are small numbers)
                        -> Model will prefer smaller sequences as the probability will be less reduced by further elements (penalty of big sequences)

                        -> normalize the result by the length of the sequence

                            1/T_y^alpha * prob
                                . alpha is tipically 0.7 (if = 0, no normlization and alpha = 1, fully normlaized)  <- heuristic

                -> Which B to choose?
                    . larger B is better (wider search, better result) but increases computional cost (bad performance and memory use)
                    . typical for production system: B = 10
                    . research/benchmark: B = 100-1000


            -- Error Analysis on Beam Seach --        
                . beam search, because is not a exact seach but instead an approxiamte search (does not search all possibilities) can be very prone to miss the best/optimal solution
                    -> when to know if the model bad accuracy is caused by the RNN model or by the beam search not being wide enough?

                RNN computes P(y|x), 
                    compare: P(y*|x) > or < P(y^|x)   (y* = real value, y^ = pred value)

                        if P(y*|x) is bigger: Beam search is failling to find the best/max fix

                        if P(y^|x) is bigger: RNN is at fault, bc the truth is that y* is better than y^ but the RNN predicted the opposite

                        note: if using lenght-normalization, compare the optimization object value instead

                
        --- Bleu Score ---
            . How to evaluate a machine translation, where there can be mutliple correct translations?

            -> Bleu Score: as long as the machine translation is close to any of the human translations, then it gets a high Score
                -> BLEU: bilingual evaluation understudy

            Precision method: count if the word appear in both the references and the machine translation output (but clip the count to the max number of times it shows in any the references)

                -> Use single words, but also bigrams (2 word sequence) and even bigger sequences

            -> This Score is great because it results in just a single real number evaluation metric!
                -> good for any sequence-to-sequence where the output is a sentence (image captioning) that has mutiple/subjective correct labels
                     

    -- Attention Model --
        . paper: Bahdanay 2014, Neural machine translation by jointly learning to align and translate
        . for long sequences, it is easier to divide the subsequence into shorter ones
            -> for machine translation, Bleu score drops as sentence length increases...but this is solved by dividing into shorter ones

        -> Use 'Attention Weights' to determine which elements of the input sequence should be taken into consideration for a certain output element
            . these weights, alpha<i, t> depende on the a<t> of the input element t and the state of the previous output element s<i-1>
                -> alpha: amount of 'attention' y<t> should pay to a<i>

        input of the output elements - c 'contact'
            contact<i> = sum_t(alpha<i, t>*a<t>)

            a<t, t'> = exp(e<t, t') / sum_T(exp(e<t, t'>))   <- a softmax for the elements e<t, t'>  
                e<t, t'> -> use small NN to compute it, to learn this function mapping:
                    input layer: s<t-1> + a<t'>  (we know it depends on these inputs)
                    FC layer
                    output: e<t, t'> (trust that the NN will find the F(s<t-1>, a<t'>))

        Note: algorithm has quadratic cost, Tx * Ty  -> quite expensive

        . this ideia can also be applied to image_caption: just pay attention to certain parts of the picture at a time

        . visualizing the 'alphas', attentions, could give nice intuition to what the output sequence is paying attention from the input sequence


    -- Speech Recognition --
        . problem: given auto clip x, find text transcript y
        . preproccess of the audio clip: applying preprocess methods like frequency/amplitude plotting is a common first step

        -> Use the 'Attention Model'
            . use CTC cost: Connectionist Temporal Classification
                input has very high frequency..output has repeated characters (so it outputs at the same frequence):
                    eg: the quick -> ttt_h_eee___s___qqq...
                    -> collpase repeated characters not seperated by '_' (blank)  -> convert the repeated to the text characters

        - Trigger Word Detection -
            . keyword detection....much easier to do (less traning data required) than proper speech Recognition
            . just training on normal speech with many words and just one trigger word will produce a very imbalaced traning set (many many negatives)
                -> hack solution:  on a sequence, when the trigger word is inputed as an element, have some of the procedding elements also be labbeled as positive
                    hey->0, ale->1, xa->1, ple->1, ase->1, tell->0, .....


                        









<<<<<<< HEAD
        
=======
                    
                     
>>>>>>> 911cd73c6729bcbcb19e5acb7af9f26a958b1688



            

