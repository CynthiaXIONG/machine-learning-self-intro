--- DEEP NEURAL NETWORK ---

    . Why are Deep NN good?
     -> Early layers detect simple features
     -> Consequent layers detect more complext features based on the composition of the previously detected simpler features
     e.g: image recgn: initially edges -> parts of the face (eye, mouth) -> faces
        speech recogn: low level audio -> phonames -> words -> sentences

    . The deeper the layer, the more complext the detected feature can become

    - Circuit Theory -
        analogy to circuit gates logic (NAND, AND, OR)
        . simpler functions can be composed into computing very complex functions (everything can be done with NANDS!!!)

    .notation:
        L -> num of layer
        n[l] -> num of units in layer 'l'. l = 0 (inputs layer), l = L (output layer)
        a[l] -> activations in layer 'l'. a[l] = g(z[l])
        w[l] -> weights for computing z[l] in layer 'l'

    . Implementation:
        . Keep dimensions correct:
            e.g from video:
                L = 5
                n[0] = 0, n[1] = 3, n[2] = 5, n[3] = 4, n[4] = 2, n[5] = 1
                z[1] = [3,1], z[l] = [n[l],1], ...
                b[1] = same as z
                w[1] = [3,2], w[l] = [n[l], n[l-1]]
                Z[l] = [n[l], m]
                dW = same dim as w
                dB = same dim as b

        Front/Back Prope:
            . Front: Calc Z[l] and A[l] (cache Z, useful in future)
            . Back: Calc dA (uses cached Z, to calc dZ), dW, dB
                dZ[l] = W[l+1] * dZ[l+1] .* g[l]'(z[l])
                dW[l] = 1/m * dZ[l] * A[l-1]
                dB[l] = 1/m * np.sum(dZ[l], axis=1, keepdims=true)
                dA[l-1] = W[l] * dZ[l]
                dZ[l-1] = ...

                dA[l] (outer layer) = d L(y_pred, y) = -(y/y_pred) + (1-y)/(1-y_pred)
            
        Hyper-Parameters:
            -> Parameters used to tune the model
            . learning rate
            . #iterations of grad descennt
            . #hidden layers, l
            . #hidden units, n
            . activation functions per layer

            -> Try values, see how it changes cost or another performance metric, try again! (visualize!, grid test, cross-valida)
                -> Imperical Proccess
            




