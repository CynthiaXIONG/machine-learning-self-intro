--- Dynamic Programming Methods - Policy and Value Iteration ---
    . Solving MDP (Bellmans optimialty equations for v*(s) and q*(s,a)) using iterative methods -> optimized by using DP

    -> DP is a solution for planning (all knowledge of the environment is known, the transitions structre of the MDP is known) not for RL

    -- Policy Evaluation - Prediction --
        . calculate v_π (state-value function) for an arbitrary policy π  (evaluate a given policy)
            v_π = sum_a(π[a|s]*sum_s',r(p[s',r|s,a]*(r+γv_π(s'))))
                - π[a|s]: prob of taking action a in state s under policy π

        Iterative Method:
        i. assign initial values arbitrarily (initial approximation)
        ii. sucessive approximations obtainde using the Bellman equation for v_π as an update rule:
            v_π_k+1 = Bellmans Eq using current v_π_k (for state s')
        iii. stop the iteration once the convergion (v_π_k+1- v_π_k) is very small (update results in very little change)

        . the reason for computing this value fuction for "a policy" is to help find better policies
    
    --  Policy Improvement --
        . from state s we would like to know wether or not we should change th e policy and choose a != π(s)
            - if the q_π(s, π'(s)) >= v_π(s) , then we should!
                . the state-action value by chosing different action is better than the current state value on the current policy

            . we should consider changes to all states and to all posiible actions, and then selecing the max (greedy) action-state value to update the new policy

                π'(s) = argmax_a(sum_s',r(p[s',r|s,a]*(r+γv_π(s')))

    -- Policy Iteration --
        . starting policy π -> improved using v_π to yield a better policy π' -> compute new v_π' and use it to get a even more improved π'' -> and so onnnnnnn
            . cycle of policy evaluation followed by policy Improvement

           ->> Policy iteration has two simulateneous processes:
            - making the value function consiste with the current policy (policy evaluation)
            - making policy greedy with respect to the current value function (policy improvement)

        . drawback: each iteration involves policy evaluation which is an iterative computation that can be expensive!

    -- Value Iteration --  
        . if the policy evaluation is done iteratively, it only converges to the real v_π in the limit (num_iterations = inf)
        . what if we dont have to have a very accurate value of the real v_π and instead stop the convergion/iteration process early?
            -> What if we stop just after one iteration/sweep of all states? the value for v_π_k where k=1 is already somewhat close to the real v_k
                -> THIS IS THE VALUE ITERATION Method

        . it can also simplify even more the Policy Iteration and combine the policy improvement and this new "shorted" policy evaluation in one step:
            v_k+1(s) = max_a(sum_s',r(p[s',r|s,a] * (r + γv_k(s'))))  
                . v_0 is arbitraty just like in the policy evaluation methods
                . the policy evaluation tries to calculate the average value using the policy over all actions, while this value iteration greedly selects the action that will return the highest state-action value
                    -> maximum over all actions instead of doing the sum of the probability weighted actions 

            -> this will yield the optimal v*(s)

        . having calculated v*, π* is just a greedy policy of the hightes state-action value per state
            π*(s) = argmax_a(sum_s',r(p[s',r|s,a] * (r + γv*(s'))))

        -> Faster convergion than Policy Iteration (as it combines the policy improvment (max of a) into the policy evaluation)


 

    


