---- Dissecting Reinforcement Learning ----
    .from: https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html

    --- Markov Chain ---
        -> stochastic processes
        . Set of possible states: S = {s0, s1, ..., sm}
        . Initial state: s0
        . Transition Model: T(s, s')
            -The future state, s', is determined only by the current/present state, s, (independent from previous states)
            - T can be defined by a transition matrix, where the values are the probablity of ending up in state s', from the current state s
            - The initial state of T at k=0 can be extrapolated to determined the k-step transition probablity (by the k-th power of the T)
                k-step_transition_prob = T^k  (snippet_1 -> rl-notes.py)
            - The vectors of the matrix represent the probability of staying in that state
        . Initial distribution: v, state of the system at k=0
        . The state probability after k-steps = Dot(v, step_transition_prob)
        . After n-steps the state probablity distribution can converge to equilibrium, independently of the starting distribution (but this not always happens)

    --- Markov Decision Process - MDP ---
        -> Markov chain based with the inclusion of an Agent and a Decision making process
        . Set of possible states: S
        . Initial state: s0
        . Set of possible actions: A
        . Transition Model: T(s, a, s')
        . Reward function: R(s)

        . T represents the probability of reaching s', from s if action 'a' is taken
        . R is a real value from the agent moving to s0

        -> Problem: maximise the reward, avoind states which return negativas values and choosing the ones which return positive values
        -> Solution: find the policy PI(s) which return s the action with the highest reward

        . PI* -> optimal poloty, which retuns the highest expected reward/utility

    --- Bellman Equation ---
    -> problem: how to choose the best policy? how to compare policies?
    . U: utility is the instant reward from state 's', plus the possible max rewards of future states  (utility is also known as Value)
        -> Disconted Rewards:  U = R(s0) + γ*R(s1) + γ^2R(s2) + .... + γ^n*R(sn)
            γ: [0:1] discount factor -> preference of the agnt for the current rewards over future rewards
        
        -> Bellman Equation: U(s) = R(s) + γ*max_a(sum(T(s,a,s') * U(s')))
            . action with the maximum sum of the utility of the possible subsequent states * the probability of getting to those subsequent states s'

        . For 'n' possible states there are 'n' Bellman equations. If the Bellman Eq was linear, this would be easily solved, but it is not (because of the MAX!!)
            -> Solution is an iterative approach

        --- Dynamic Programming Algorithms ---
        -- Value Iteration Algorithm -- snippet_3
            . the Bellman Eq is the iteration algorithm to solve a MDP
            . find the utility/value for each state, through an iterative approach
                i. start with arbitary values (0)
                ii. Bellman Update: calculate the utility of a state and update it
                iii. updating will converge to equilibrium (only reached in the inf iteration)
                iv. stop updating when convergion from one iteration to the next is very small
                    ||U(k+1) -U(k)|| <  ϵ * (1 - γ) / γ  
                    . ϵ: stopping criteria value
                    

        -- Policy Iteration Algorithm -- snippet_4
            . find the optimal policy that maximizes the reward, PI*
            . for a given action, a, evaluate this policy utility/value, using the Bellmans Eq (but without the need for iterating the actions to find a max, because we want the utility for a selected "policy action")
            . policy improvement: improve policy with the action that returns the best utility


        -- Value VS Policy --
            . Choose Policy iteration if:
                -many actions
                -start from a fair policy
            . Choose Value:
                -few actions and transition is acyclic 
    

    --- Model-Free Reinforcement Learning ---
        . there is no transition model, T
        . there is no reward function, R(s)

        -- Passive RL --
        . policy , PI, always produce the action ,a, for the agent to use
        . The goal of the agent it to learn the Utility function, U
            -> MC for prediction

        -- Active RL --
        . estimate the optimal policy while moving in the environment
            -> MC for control estimation

        i. Transition Model estimation:
            . take actions in the environment and keep track of the final states and its probablities.
        ii. Utility Function estimation: 
            . with the Transition Model available, use value or policy iteration to extimate the utility function
        
        !problem: Estimating the transitional model can be very expensive (proportional to the state and action space) and some states can be extremely unlikely
            -> Solution: Monte Carlo Method, which directly estimates the utility function without the need of a transtion model

    --- Monte Carlo Method - MC ---
        . using randomness to solve porblems
        . advantages vs dynamic programming:
            -MC allows learning optimal policy direclty from interaction with environment
            -easy and efficient to focus MC methods on a small subset of states
            -MC can be used with simulations (sample models)  -> ability to simulate episodes and use this data

        . peseudo-algorithm of MC Prediction - Passive:
            . transitional 
            i. start from initial state and follow internal policy
            ii. store history of all states visited untill reaching terminal state -> Episode
            iii. calulate the U based on the bellmans discounted reward in each state
            iv. iterate untill convergion!! (NOTE: It is garanteed to converge)

        . visit: each occurence of state during the episodes
        -> First Visit MC: U(s) is the avg of the utiliy following the FIRST visit to 's' in a set of episodes
        -> Every Visit MC: U(s) is the avg of EVERY/ALL visits to 's'

        . The MC method can happen to never reach certain states. This can be an advantage as we will only estimate the utilities, U, for the states we are interested in
            -> rl_notes.py, snippet_6 with exploring_starts=False
                For this cause, this is solved by starting at random places to make sure all states are reached
        

        -- Monte Carlo Control - Active --
            . policy is not given, so need to be estimated
            . similar to the dynamic programming approach: Policy iteration
                -> GPI - Generalized Policy Iteration
                    i. Policy evaluation: U -> U(PI) (get utility based on action given by the policy PI)
                    ii. Policy Improvement: PI -> greedy(U) (update the policy for a state based on the best action)
                        -> greedy algorithm: local optimal is chosen at each step of the iteration, but because the choice is based on the utility function, U, which is adjusted along time it will not bu stuck in local optimals, as these will change

            
            - Q-Function <-> Action-Value Function -
                . takes the action 'a' in state 's' under policy 'PI' and returns the utility of that state-action pair
                -> primary goal of MC Control: estimate Q
                    . State-Actio matrix Q, stores the utility of executing a specific action in a specific state
                    . Query along the actions of a state should give us the policy (choose the highest utility action, greedly)
                        PI(s) = argmax Q(s,a)
                        . This might lead to not exploring diferent actions when the table starts to get updated (if Q values are default to 0, and update starting adding some positive values, those will be selected instead of the unknown action,state with Q=0)

                        -Solution: Exploring starts
                            . make sure the first step of each eosode starts at a state-action pair where all possible actions have a non-zero probability

                        rl-notes.py -> snippet_7

                ->> MC always converges to Optimal Policy
                    . when it is convergint to a sub-optimal policy, the utility function would converge to the utility function for that policy and that would cause the policy to change
                    . Stability is only reached when BOTH Policy and Utility functions are optimal

                . Having to save a full epsiode before updating/learning is a strong limitation
                    . This could be a serious issue if episodes can take very long to be completed or never end (cycles!!)
                    -> Temporal Differencing Learning (TD) overcomes this by updating after a single step 
                    

    --- Temporal Differencing (TD) ---
    . Update the utility function, U, after each visit
        -> results in not having all the states and their 'utility' values
        -> the only available information is r(t+1) and the U estimated for the future step, t+1

        -- TD(0) --
            . simplest form of the TD learning
            . passive learning -> model-free prediction -> utility function prediction

            Update Rule: U(s_t) = U(s_t) + alpha(r_t+1 + gamma*U(s_t+1) - U(s_t))
                -> alpha = learning rate or step size [0-1], how much to update the new utility based on the estimation error of the U_t+1
                    -> New Estimate = Old_estimate + alpha*(Target - Old_estimate)

            -> TD(0) ignores the past states, just takes into accound the current state
            -> implementation: snippet_8

        -- TD(labmda) --
            . extend the utility_matrix update, with the new state, t+1, estimation 'learning' to more of the precceding states than just the current state (TD(0))
            . updates the utility_matrix way faster than TD(0), bc TD can only propagate the learning by one state at a time per step
                -->> CONVERGES FASTER!!

            . short-term 'memory' of states visited in the last step -> trace_matrx
            . labmda -> trace decay, defines the 'weight' of each state visited before
                -> labmda = 0, prev states are not taken into account, TD(0)
                -> labmda = [0-1], weight of the traces decay over time, giving more weight to the last states
                -> labmda = 1, all precceding states are equally updated
                    . this one is kind of similar to the MC method as it updates all the states (but with updates after every step and not only after the epsiode is done)

            - Update Rule -
                . U_t(s) = U_t(s) + alpha * delta
                . delta = delta_TD(0) * e_t(s)     

                . e_t(s) -> trace matrix
                . e_t(s) = gamma * labmda * e_t-1(s)

                -> implementation: snippet_9

    --- SARSA: Temporal Differencing Control ---
        -> Extending TD(0) to active learning -> control case -> optimal policy estimation
            -> Use Q-function to esmate best policy (very similar to TD(0) update rule)

            Q(s_t, a_t) += alpha * (r_t+1 + gamma*(Q(s_t+1, a_t+1) - Q(s_t, a_t)))

                . The difference is that for Q needs the tuple [state, action] (while U is only a function of 's')
                    -> Need to predict the best future action, a_t+1, which we can select from the current policy PI(s_t+1)

             RARSA -> TD control estimation is based on tuple [State0, Actiom0, Reward, State1, Action1]

        - Algorithm -
            i. move one step by selection action, a_t, from policy, PI(s)
            ii. get the new state, s_t+1, reward and select the associated action, a_t+1 from PI(s_t+1)
            iii. update the Q-function
            iv. Update the policy, PI(s), by greedy selection of the action corresponding to the Q(s, a) that has the highest value (upadte at each step)

            -> implementation: snippet_10

        -- SARSA(labmda) --
            . we can use the same algorithm as in TD(labmda) to speed the convergence
            . Uses the eligibility traces to propagate the learning and update the Q of the states previously visited (with the decaying factor)

            ->  Q(s_t, a_t) += alpha * alpha * delta * trace_matrix

        . SARSA always converges to the optimal policy if all the state-action pairs are visited an infinite number of times
        . The infinte issue can be tackled by using random exploration!
            -> Epsilon: probability of chosing random action
                . if epsilon is too high the convergion will be slow because of over exploration
                . too low can fail to garantee that every state-action pairs will be visited/tested

    --- Q-Learning: Off-policy ---
        . on-policy learning: use PI policy to learn on the fly, updating PI from experiences sampled from PI (previous TD methods)
        . off-policy learning: no need for PI to update Q
            -> learning by observation
                . learn optimal policy from sub-optimal policy (could be a random one)
                . allow re-using old experiences generated from old policies to improved current policy
                    -> Experience Relay!

        - Update Rule -
            . similar to SARSA but the 'target'/'new state estimation' is not based off the Q of the action, a_t+1, for s_t+1 given by current policy (does not follow the Generalized Policy Iteration algorithm/cycle)
                -> estimation is based on the max possible Q for state s_t+1
                    . the estimation does not follow a policy -> off policy, and instead try to use the best possible action, a_t+1, for the estimation
                        exploration policy, PI(s_t+1) = argmax Q(s_t+1, a) or random action
                

                -> Target/Estimation = r_t+1, gamma * max_a Q(s_t+1, a)

        - Algorithm -
            i. move one step, selecting a_t from the exploration policy, PI
            ii. observe r_t+1, s_t+1
            iii. update the q_matrix

            -> Q-Learning updates q_matrix with the tuple [s0, a0, r, s1]
                . a1 is not considered (unlike SARSA)

        - Q(labmda)-Learning -
            . based on the eligibility trace matrix, similar to TD(labmda) and SARSA(labmda)
            . because of the exploration policy, don't propagate the Q update to the previous states if the action used was the random exploration one
                . trace_matrix = labmda * gamma * trace_matrix only if Q_t-1 = max_a Q_t-1    
                    otherwise = 0

            -> cutting off traces when exploratory non-greedy action is taken loses much of advantages of the eligibility lambda trace (faster convergence)
    
    Conclusion: TD methods converge faster than MC methods!
        




                


            






            


        


            
